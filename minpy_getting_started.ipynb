{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with minpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to create your own optimization algorithm based on buiding blocks from minpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import minpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example we show how to apply the existing optimization algorithm from minpy to minimize a function of two variables. \n",
    "First we define the starting point for the search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X0 = np.array([0.8,1.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use sphere function from our set of testing functions to test the algorithm.Several testing functions are defined in module optfun.  Add your test function there or import from another Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optfun as of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of minimization class, and call the optimization method (minimize_NM is stochastic extention of Nelder and Mead algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclass = minpy.Minimization(of.spher,X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = myclass.minimize_NM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we discuss optimization algorithms similar to original Nelder and Mead alorithm (NM). First variant (NM-stochastic) is very similar to NM but corrects some of its drawbacks, and second variant (NM-nonlocal) has some similarity to random search and helps to resolve some other issues of classical NM algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of NM-stochastic:\n",
    "1. Initialize the search by generating $K\\geq n$ separate realizations of $u_0^i$, i=1,..K of the random vector $U_0$,set $m_0=\\frac{1}{K} \\sum_{i=0}^{K} u_0^i$\n",
    "\n",
    "2. On step j = 1, 2, ...\n",
    "\n",
    "a.Compute the mean level $c_{j-1}=\\frac{1}{K} \\sum_{i=1}^K f(u_{j-1}^i)$\n",
    "\n",
    "b.Calculate a new set of vertices:\n",
    "\n",
    "$$u_j^i= m_{j-1}+\\epsilon_{j-1} (f(u_{j-1}^i)-c_{j-1})\\frac{  m_{j-1} -u_{j-1}^i}  {||m_{j-1} -u_{j-1}^i ||^n }$$\n",
    "\n",
    "c.Set  $m_j=\\frac{1}{K} \\sum_{i=0}^K u_j^i$\n",
    "\n",
    "d.Adjust the step size $\\epsilon_{j-1}$ so that $f(m_j)<f(m_{j-1})$. If approximate $\\epsilon _{j-1}$ cannot be obtained within the specified number of trails, then set $m_k=m_{j-1}$ \n",
    "\n",
    "e.Use sample standard deviation as termination criterion: $D_j=(\\frac{1}{K-1} \\sum_{i=1}^K (f(u_j^i)-c_j)^2)^{1/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that classic simplex search methods do not use values of objective function to calculate reflection/expantion/contraction coefficients. Those coefficients are the same for all vertices, whereas in NM-stochastic the distance each vertex will travel depends on the difference between objective function value and average value across all vertices $(f(u_j^i)-c_j)$\n",
    "In the above algorithm simlex may collapse into a nearly degenerate figure. The usually proposed remedy is to restart the simlex every once in a while.  Note that only initial vertices are randomly generated, and the path of all subsequent vertices is deterministic. \n",
    "In the next variant of the algorithm we will maintain the randomness of vertices on each step, while adjusting the distribution of $U_0$ to mimic the pattern of the modified vertices. The corrected algorithm has much higher exploration power than the first algorithm (similar to the exploration power of random search algorithms), and has exploitation power of pattern search algorithms.\n",
    "\n",
    "\n",
    "Steps of NM - nonlocal\n",
    "\n",
    "1. Choose a starting point $x_0$ and set $m_0=x_0$. Compute $(fm_0).$\n",
    "\n",
    "2. On step j = 1, 2, ...\n",
    "Obtain K separate realizations of $u_i^i$, i=1,..K of the random vector $U_j$\n",
    "\n",
    "a.Compute $f(u_{j-1}^i) , j = 1,2,..K$, and the sample mean level $$c_{j-1}=\\frac{1}{K} \\sum_{i=1}^K f(u_{j-1}^i)$$\n",
    "\n",
    "b.Generate the new estimate of the mean:\n",
    "\n",
    "$$m_{j}= m_{j-1}+\\epsilon_{j}\\frac{1}{K} \\sum_{i=1}^K[(f(u_{j}^i)-c_{j})\\frac{  m_{j-1} -u_{j}^i}  {||m_{j-1} -u_{j}^i ||^n }]$$\n",
    "\n",
    "Adjust the step size $\\epsilon_{j-1}$ so that $f(m_j)<f(m_{j-1})$. If approximate $\\epsilon _{j-1}$ cannot be obtained within the specified number of trails, then set $m_k=m_{j-1}$ \n",
    "\n",
    "c.Use sample standard deviation as termination criterion: $$D_j=(\\frac{1}{K-1} \\sum_{i=1}^K (f(u_j^i)-c_j)^2)^{1/2}$$\n",
    "\n",
    "references\n",
    "https://www.damtp.cam.ac.uk/user/reh10/lectures/nst-mmii-chapter2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstration shows how to create custom algorithms using basic modules from minpy. We create NM-stochastic algorithm as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NM-stochastic will be constructed using the following building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization: \n",
    " Generating $K\\geq n$ separate realizations of $u_0^i$, i=1,..K of the random vector $U_0$ \n",
    " update m: Compute  center of mass of initial simplex \n",
    " $$m_0=\\frac{1}{K} \\sum_{i=0}^{K} u_0^i$$ \n",
    "update c: Compute mean level of objective \n",
    " $$c_{j-1}=\\frac{1}{K} \\sum_{i=1}^K f(u_{j-1}^i)$$\n",
    "update simplex: Compute new set of vertices $$u_j^i= m_{j-1}+\\epsilon_{j-1} (f(u_{j-1}^i)-c_{j-1})\\frac{  m_{j-1} -u_{j-1}^i}  {||m_{j-1} -u_{j-1}^i ||^n }$$\n",
    "update m: Compute  center of mass at step $j$\n",
    " $$m_j=\\frac{1}{K} \\sum_{i=0}^{K} u_j^i$$\n",
    "\n",
    "adjust step: Adjust the step size $\\epsilon_{j-1}$ so that $f(m_j)<f(m_{j-1})$. If approximate $\\epsilon _{j-1}$ cannot be obtained within the specified number of trails, then set $m_k=m_{j-1}$ \n",
    "\n",
    "stop: Check termination criterion - sample standard deviation: $$D_j=(\\frac{1}{K-1} \\sum_{i=1}^K (f(u_j^i)-c_j)^2)^{1/2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minpy import Minimization\n",
    "\n",
    "class NM_Minimization(Minimization):\n",
    "    \n",
    "    def update_simplex(self):\n",
    "        pass\n",
    "       \n",
    "    def NM_stochastic(self):\n",
    "        self.initialize()\n",
    "        self.update_m()\n",
    "        while not self.stop():\n",
    "            self.update_c()\n",
    "            self.update_simplex()\n",
    "            self.update_m()\n",
    "            self.adjust_step()\n",
    "        \n",
    "        return self.get_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we apply this algorithm to Sphere function in 3d and to Shor function in 5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sphere optimal function value: 0.0  at X: 0.522 2.271\n",
      "***********\n",
      "shor optimal function value: 0.0  at X: 0.522 2.271 2.271 -0.793 1.619 2.088\n",
      "***********\n"
     ]
    }
   ],
   "source": [
    "from optfun import spher,shor\n",
    "\n",
    "X0 = np.array([0.8,1.9])\n",
    "\n",
    "onSphere = NM_Minimization(spher,X0)\n",
    "res = onSphere.NM_stochastic()\n",
    "print('sphere optimal function value:', round(res[0],3), ' at X:', round(res[1][0],3)   ,round(res[1][1],3)) \n",
    "print('***********')\n",
    "\n",
    "X0 = np.array([0.8,1.9,-0.5,1.2,2.1])\n",
    "\n",
    "onShor = NM_Minimization(shor,X0)\n",
    "res = onShor.NM_stochastic()\n",
    "print('shor optimal function value:', round(res[0],3), ' at X:', round(res[1][0],3)   ,round(res[1][1],3),\n",
    "     round(res[1][1],3), round(res[1][2],3), round(res[1][3],3), round(res[1][4],3)) \n",
    "print('***********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
